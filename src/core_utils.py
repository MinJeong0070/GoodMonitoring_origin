# core_utils.py
import re
from html import unescape
try:
    import kss  # ì„ íƒ: ì„¤ì¹˜ë˜ì–´ ìˆìœ¼ë©´ ì‚¬ìš©
    _HAS_KSS = True
except Exception:
    _HAS_KSS = False
import difflib
import os
import time
import psutil
import urllib.parse
import logging
import requests
import pandas as pd

from bs4 import BeautifulSoup
from konlpy.tag import Okt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
from datetime import datetime
from urllib.parse import urlparse

from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options

# ë‚ ì§œ
today = datetime.now().strftime("%y%m%d")

# ë“œë¼ì´ë²„ ê²½ë¡œ (ë³¸ì¸ PCì— ë§ê²Œ ìˆ˜ì •)
driver_path = r"C:\chromedriver-win64\chromedriver.exe"

okt = Okt()

# ì œì™¸ ë„ë©”ì¸ ë¡œë“œ
excluded_domains = pd.read_excel("../../excel/ìˆ˜ì§‘ ì œì™¸ ë„ë©”ì¸ ì£¼ì†Œ.xlsx")["ì œì™¸ ë„ë©”ì¸ ì£¼ì†Œ"].dropna().tolist()

# ë¡œê·¸ ì„¤ì •
os.makedirs("../../log", exist_ok=True)
logging.basicConfig(
    filename=f"../../log/ë¡œê·¸_{today}.txt",
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    encoding='utf-8'
)

def log(msg, index=None):
    prefix = f"[{index+1:03d}] " if index is not None else ""
    full_msg = f"{prefix}{msg}"
    print(full_msg)
    logging.info(full_msg)

def create_driver(index=None):
    try:
        options = Options()
        options.add_argument("--headless=new")
        options.add_argument("--disable-background-networking")
        options.add_argument("--disable-sync")
        options.add_argument("--disable-default-apps")
        options.add_argument("--no-first-run")
        options.add_argument("--disable-gpu")
        options.add_argument("--no-sandbox")
        options.add_argument("--disable-dev-shm-usage")
        options.add_argument("--disable-extensions")
        options.add_argument("--disable-blink-features=AutomationControlled")
        options.add_argument("user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/135.0.0.0 Safari/537.36"
)

        prefs = {
            "profile.managed_default_content_settings.images": 2,
            "profile.default_content_setting_values.media_stream": 2,
            "profile.default_content_setting_values.notifications": 2,
            "profile.default_content_setting_values.geolocation": 2,
            "profile.default_content_setting_values.media_stream_mic": 2,
            "profile.default_content_setting_values.media_stream_camera": 2,
        }
        options.add_experimental_option("prefs", prefs)
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option("useAutomationExtension", False)

        driver = webdriver.Chrome(service=Service(driver_path), options=options)
        driver.execute_cdp_cmd("Page.addScriptToEvaluateOnNewDocument", {
            "source": "Object.defineProperty(navigator, 'webdriver', { get: () => undefined });"
        })

        return driver
    except Exception as e:
        log(f"âŒ ë“œë¼ì´ë²„ ìƒì„± ì‹¤íŒ¨: {e}", index)
        return None



def kill_driver(driver, index=None):
    if driver:
        try:
            driver.quit()
        except Exception as e:
            log(f"âš ï¸ driver.quit() ì‹¤íŒ¨: {e}", index)

        try:
            pid = getattr(driver.service.process, 'pid', None)
            if pid and psutil.pid_exists(pid):
                parent = psutil.Process(pid)
                children = parent.children(recursive=True)
                for child in children:
                    child.kill()
                parent.kill()
                log(f"ğŸ’€ ChromeDriver PID {pid} ê°•ì œ ì¢…ë£Œ ì™„ë£Œ (ìì‹ {len(children)}ê°œ)", index)
        except Exception as e:
            log(f"âš ï¸ ê°•ì œ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì‹¤íŒ¨: {e}", index)


def clean_text(text):
    if not isinstance(text, str):
        text = str(text)

    # "nan" ë¬¸ìì—´ ì œê±°
    if text.strip().lower() == 'nan':
        return ""

    #  ë¹„ë””ì˜¤ í”Œë ˆì´ì–´ ê´€ë ¨ ì‹œìŠ¤í…œ ë¬¸êµ¬ ì œê±°
    patterns_to_remove = [
        r"Video Player",  # "Video Player" ë¼ì¸
        r"Video íƒœê·¸ë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¸Œë¼ìš°ì €ì…ë‹ˆë‹¤\.",  # ì•ˆë‚´ ë¬¸êµ¬
        r"\d{2}:\d{2}",  # 00:00 í˜•ì‹ ì‹œê°„
        r"[01]\.\d{2}x",  # ì¬ìƒì†ë„ (ì˜ˆ: 1.00x)
        r"ì¶œì²˜:\s?[^\n]+",  # ì¶œì²˜: KBS News ë“±
        r"/\s?\d+\.?\d*",   # / 2 ë˜ëŠ” / 2.00 í˜•íƒœê¹Œì§€ ëª¨ë‘ ì œê±°
        r"Your browser does not support the video tag."
    ]
    for pattern in patterns_to_remove:
        text = re.sub(pattern, "", text)

    #  íŠ¹ìˆ˜ ë¬¸ì ë° ì´ìŠ¤ì¼€ì´í”„ ì •ë¦¬
    text = text.replace("\\\"", "\"").replace("\\'", "'").replace("\\\\", "\\")
    text = re.sub(r"[ã…‹ã…ã… ã…œ]+", "", text)
    text = re.sub(r"[!?~\.\,\-#]{2,}", "", text)

    #  HTML ì—”í‹°í‹° ë° íŠ¹ìˆ˜ ë¬¸ì ì œê±°
    text = re.sub(r"&[a-z]+;|&#\d+;", "", text)

    #  ê³µë°± ë° ì œì–´ ë¬¸ì ì •ë¦¬
    text = re.sub(r"[\\\xa0\u200b\u3000\u200c_x000D_]", " ", text)
    text = re.sub(r"\s+", " ", text)

    return text.strip()

#í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords(text, num_keywords=5):
    nouns = okt.nouns(text)
    return " ".join(nouns[:num_keywords])

#ë¬¸ì¥ ì¶”ì¶œ
def extract_first_sentences(text):
    paras = re.split(r'\n{2,}', text.strip())
    get_first = lambda p: re.split(r'(?<=[.!?])(?=\s|[ê°€-í£])', p.strip())[0] if p else ""
    get_last = lambda p: re.split(r'(?<=[.!?])(?=\s|[ê°€-í£])', p.strip())[-1].strip() if p else ""
    first = get_first(paras[0]) if len(paras) > 0 else ""
    second = get_first(paras[1]) if len(paras) > 1 else ""
    last = get_last(paras[-1]) if len(paras) > 0 else ""

    return first, second, last

#ë¬¸ì¥ ìœ ì‚¬ë„ í™•ì¸í•¨ìˆ˜
def calculate_copy_ratio(article, post):
    def clean(t): return re.sub(r'\s+', ' ', re.sub(r'[^\w\s]', '', t)).strip()
    article, post = clean(article), clean(post)
    sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', article) if s.strip()]
    if not sentences: return 0.0
    scores = []
    for s in sentences:
        try:
            v = TfidfVectorizer().fit([s, post])
            tfidf = v.transform([s, post])
            scores.append(cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0])
        except:
            continue
    return round(sum(scores)/len(scores), 3) if scores else 0.0

#ê°™ì€ ë¬¸ì¥ ìœ ë¬´ í™•ì¸í•¨ìˆ˜
def similar_sentence(article, post, threshold=0.3):

    article_sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', article) if s.strip()]
    post_sentences = [s.strip() for s in re.split(r'(?<=[.!?])\s+', post) if s.strip()]

    for art_s in article_sentences:
        for post_s in post_sentences:
            try:
                vectorizer = TfidfVectorizer().fit([art_s, post_s])
                tfidf = vectorizer.transform([art_s, post_s])
                sim = cosine_similarity(tfidf[0:1], tfidf[1:2])[0][0]
                if sim >= threshold:
                    return True
            except:
                continue
    return False


def safe_get(driver, url, timeout=90, index=None):
    try:
        driver.set_page_load_timeout(timeout)
        driver.get(url)
        return True
    except Exception as e:
        log(f"â° safe_get í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨: {url} ({e})", index)

        if "connection refused" in str(e).lower() or "connection aborted" in str(e).lower():
            log("ğŸ’¥ ë“œë¼ì´ë²„ ì„¸ì…˜ì´ ì£½ì—ˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.", index)
            kill_driver(driver, index)
            return False

        kill_driver(driver, index)
        return False

def load_trusted_oids():
    def load_oid_from_excel(filename):
        try:
            # í•µì‹¬: int â†’ str â†’ zfill(3)
            return set(
                pd.read_excel(filename)["oid"]
                .dropna()
                .astype(int)
                .astype(str)
                .apply(lambda x: x.zfill(3))
            )
        except Exception as e:
            log(f"âš ï¸ {filename} ë¡œë”© ì‹¤íŒ¨: {e}")
            return set()

    base_path = "../../oid ë¦¬ìŠ¤íŠ¸"  # í´ë” ê²½ë¡œì— ë§ê²Œ ìˆ˜ì •A
    news_oids = load_oid_from_excel(os.path.join(base_path, "ë„¤ì´ë²„ë‰´ìŠ¤ ì‹ íƒì–¸ë¡  oid.xlsx"))
    sports_oids = load_oid_from_excel(os.path.join(base_path, "ë„¤ì´ë²„ìŠ¤í¬ì¸  ì‹ íƒì–¸ë¡  oid.xlsx"))
    entertain_oids = load_oid_from_excel(os.path.join(base_path, "ë„¤ì´ë²„ì—”í„° ì‹ íƒì–¸ë¡  oid.xlsx"))

    return news_oids, sports_oids, entertain_oids

trusted_news_oids, trusted_sports_oids, trusted_entertain_oids = load_trusted_oids()


def fallback_with_requests(url):
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        res = requests.get(url, headers=headers, timeout=10)
        if res.status_code != 200:
            return ""
        soup = BeautifulSoup(res.text, "html.parser")
        content_div = soup.select_one("#dic_area, article")
        if content_div:
            return content_div.get_text(strip=True)
        paragraphs = soup.find_all("p")
        return "\n".join(p.get_text(strip=True) for p in paragraphs)
    except:
        return ""

def get_news_article_body(url, driver, max_retries=2, index=None):
    for attempt in range(max_retries):
        try:
            if not safe_get(driver, url, timeout=90, index=index):
                log(f"â° get_news_article_body ë¡œë”© ì‹¤íŒ¨ (ì‹œë„ {attempt+1}) â†’ ë“œë¼ì´ë²„ ì¬ìƒì„±", index)
                kill_driver(driver, index)
                driver = create_driver(index)
                if driver is None:
                    log("âŒ ë“œë¼ì´ë²„ ì¬ìƒì„± ì‹¤íŒ¨", index)
                    return "", None
                continue

            time.sleep(1)
            soup = BeautifulSoup(driver.page_source, "html.parser")
            domain = urllib.parse.urlparse(url).netloc

            selector_map = {
                    "n.news.naver.com": "article#dic_area",
                    "m.sports.naver.com": "div._article_content",
                    "m.entertain.naver.com": "article#comp_news_article div._article_content",

                    "edaily.co.kr": "div.news_body", # 1 ì´ë°ì¼ë¦¬
                    "mt.co.kr": "div#textBody", # 2 ë¨¸ë‹ˆíˆ¬ë°ì´
                    "fnnews.com": "div#article_content",  # 3 íŒŒì´ë‚¸ì…œë‰´ìŠ¤
                    "khan.co.kr": "div#articleBody", # 4 ê²½í–¥ì‹ ë¬¸
                    "sedaily.com": "div.article_view", # 5 ì„œìš¸ê²½ì œ
                    "dailian.co.kr": "div.article", # 6 ë°ì¼ë¦¬ì•ˆ
                    "news.bizwatch.co.kr": "div.news_body.new_editor", # 7 ë¹„ì¦ˆì›Œì¹˜
                    "asiae.co.kr": "div#txt_area",  # 8 ì•„ì‹œì•„ê²½ì œ
                    "kmib.co.kr": "div#articleBody", # 9 êµ­ë¯¼ì¼ë³´
                    "biz.heraldcorp.com": "article#articleText", #10 í—¤ëŸ´ë“œê²½ì œ
                    "newspim.com": "div#news-contents", #11 ë‰´ìŠ¤í•Œ
                    "hani.co.kr": "div.article-text", #12 í•œê²¨ë ˆ
                    "nocutnews.co.kr": "div#pnlContent", #13 ë…¸ì»·ë‰´ìŠ¤
                    "ytn.co.kr": "div#CmAdContent",  #14 YTN
                    "segye.com": "div#article_txt", #15 ì„¸ê³„ì¼ë³´
                    #"hankookilbo.com": "div.col-main", #16 í•œêµ­ì¼ë³´
                    "seoul.co.kr": "div.viewContent.body18.color700", #17 ì„œìš¸ì‹ ë¬¸
                    "imbc.com": "div.news_txt", #18 MBC
                    "cctimes.kr": "div#article-view-content-div", #19 ì¶©ì²­íƒ€ì„ì¦ˆ
                    "busan.com": "div.article_content", #20 ë¶€ì‚°ì¼ë³´
                    "sbs.co.kr": "div.text_area", #21 SBS
                    "kbs.co.kr": "div#cont_newstext", #22 KBS
                    "etoday.co.kr": "div.articleView", #23 ì´íˆ¬ë°ì´
                    "breaknews.com": "div#CLtag", #24 BreakNews
                    "koreaherald.com": "article#articleText", #25 ì½”ë¦¬ì•„í—¤ëŸ´ë“œ
                    "incheonilbo.com": "article#article-view-content-div", #26 ì¸ì²œì¼ë³´
                    "etnews.com": "div#articleBody", #27 ì „ìì‹ ë¬¸
                    "kookje.co.kr": "div.news_article", #28 êµ­ì œì‹ ë¬¸
                    "ajunews.com": "div#articleBody", #29 ì•„ì£¼ê²½ì œ
                    "imaeil.com": "div#articlebody", #30 ë§¤ì¼ì‹ ë¬¸
                    "kyeonggi.com": "div.article_cont_wrap", #31 ê²½ê¸°ì¼ë³´
                    "ggilbo.com": "article.article-veiw-body", #32 ê¸ˆê°•ì¼ë³´
                    "domin.co.kr": "div#article-view-content-div",#33 ì „ë¶ë„ë¯¼ì¼ë³´
                    "asiatoday.co.kr": "div#font", #34 ì•„ì‹œì•„íˆ¬ë°ì´
                    "kado.net": "article.article-veiw-body", #35 ê°•ì›ë„ë¯¼ì¼ë³´
                    "mbn.co.kr": "div#newsViewArea", #36 MBN
                    "ksilbo.co.kr": "article.article-veiw-body", #37 ê²½ìƒì¼ë³´
                    "joongboo.com": "article.article-veiw-body", #38 ì¤‘ë¶€ì¼ë³´
                    "jbnews.com": "article.article-veiw-body", #39 ì¤‘ë¶€ë§¤ì¼
                    "kwangju.co.kr": "div#joinskmbox", #40 ê´‘ì£¼ì¼ë³´
                    "kwnews.co.kr": "div#articlebody", #41 ê°•ì›ì¼ë³´
                    "economist.co.kr": "div#article_body", #42 ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸
                    "sports.khan.co.kr": "div#articleBody",#43 ìŠ¤í¬ì¸ ê²½í–¥
                    "kgnews.co.kr": "div#news_body_area", #44 ê²½ê¸°ì‹ ë¬¸
                    "nongmin.com": "div.news_txt.ck-content", #45 ë†ë¯¼ì‹ ë¬¸
                    "yeongnam.com": "article.article-news-box", #46 ì˜ë‚¨ì¼ë³´
                    "sisain.co.kr": "article.article-veiw-body", #47 ì‹œì‚¬IN
                    "isplus.com": "div#article_body", #48 ì¼ê°„ìŠ¤í¬ì¸ 
                    "inews365.com": "div.article", #49 ì¶©ë¶ì¼ë³´
                    "daejonilbo.com": "article.article-veiw-body", #50 ëŒ€ì „ì¼ë³´
                    "kihoilbo.co.kr": "article.article-veiw-body", #51 ê¸°í˜¸ì¼ë³´
                    "newspenguin.com": "article.article-veiw-body", #52 ë‰´ìŠ¤í­ê·„
                    "mediatoday.co.kr": "article.article-veiw-body", #53 ë¯¸ë””ì–´ì˜¤ëŠ˜
                    "mdilbo.com": "div.article_view", #54 ë¬´ë“±ì¼ë³´
                    "kyeongin.com": "div#article-body", #55 ê²½ì¸ì¼ë³´
                    "gnnews.co.kr": "div.news_text", #56 ê²½ë‚¨ì¼ë³´
                    "sportsseoul.com": "div#article-body", #57 ìŠ¤í¬ì¸ ì„œìš¸
                    "idaegu.co.kr": "div.news_text", #58 ëŒ€êµ¬ì‹ ë¬¸
                    "idaegu.com": "article.article-veiw-body", #59 ëŒ€êµ¬ì¼ë³´
                    "idomin.com": "article.article-veiw-body", #60 ê²½ë‚¨ë„ë¯¼ì¼ë³´
                    "namdonews.com": "article.article-veiw-body", #61 ë‚¨ë„ì¼ë³´
                    "obsnews.co.kr": "article.article-veiw-body", #62 OBS
                    "kyongbuk.co.kr": "article.article-veiw-body", #63 ê²½ë¶ì¼ë³´
                    "knnews.co.kr": "div.cont_cont", #64 ê²½ë‚¨ì‹ ë¬¸
                    "sports.hankooki.com": "article.article-veiw-body", #65 ìŠ¤í¬ì¸ í•œêµ­
                    "jjan.kr": "div.article_txt_container", #66 ì „ë¶ì¼ë³´
                    "joongdo.co.kr": "div#font", #67 ì¤‘ë„ì¼ë³´
                    "hidomin.com": "div#article-view-content-div", #68 ê²½ë¶ë„ë¯¼ì¼ë³´
                    "naeil.com": "div.article-view", #69 ë‚´ì¼ì‹ ë¬¸
                    "kjdaily.com": "div#content", #70 ê´‘ì£¼ë§¤ì¼ì‹ ë¬¸
                    "cctoday.co.kr": "article.article-veiw-body", #71 ì¶©ì²­íˆ¬ë°ì´
                    "jnilbo.com": "div#content", #72 ì „ë‚¨ì¼ë³´
                    "viva100.com": "div.news_content", #73 ë¸Œë¦¿ì§€ê²½ì œ
                    "sportsworldi.com": "article.viewBox2", #74 ìŠ¤í¬ì¸ ì›”ë“œ
                    "sjbnews.com": "span.news_text.cl6.p-b-25", #75 ìƒˆì „ë¶ì‹ ë¬¸
                    "dynews.co.kr": "article.article-veiw-body", #76 ë™ì–‘ì¼ë³´
                    "iusm.co.kr": "article.article-veiw-body", #77 ìš¸ì‚°ë§¤ì¼
                    "dnews.co.kr": "div.text", #78 eëŒ€í•œê²½ì œ
                    "hellodd.com": "article.article-veiw-body", #79 í—¬ë¡œë””ë””
                    "ilyo.co.kr": "div.contentView.ctl-font-ty2.editorType2", #80 ì¼ìš”ì‹ ë¬¸
                    "ccdailynews.com": "article.article-veiw-body", #81 ì¶©ì²­ì¼ë³´
                    "djtimes.co.kr": "article.article-veiw-body", #82 ë‹¹ì§„ì‹œëŒ€
                    "hkbs.co.kr": "article.article-veiw-body", #83 í™˜ê²½ì¼ë³´
                    "h21.hani.co.kr": "div.arti-txt.0", #84 í•œê²¨ë ˆ21
                    "ihalla.com": "div.article_txt", #85 í•œë¼ì¼ë³´
                    "ulsanpress.net": "article.article-veiw-body", #86 ìš¸ì‚°ì‹ ë¬¸
                    "jejunews.com": "div#article-view-content-div", #87 ì œì£¼ì¼ë³´
                    "wonjutoday.co.kr": "article.article-veiw-body", #88 ì›ì£¼íˆ¬ë°ì´
                    "kbmaeil.com": "div.news_content", #89 ê²½ë¶ë§¤ì¼ì‹ ë¬¸
                    "weekly.hankooki.com": "article.article-veiw-body", #90 ì£¼ê°„í•œêµ­
                    "yjinews.com": "article.article-veiw-body", #91 ì˜ì£¼ì‹œë¯¼ì‹ ë¬¸
                    "ebn.co.kr": "article.article-veiw-body", #92 EBNì‚°ì—…ë‰´ìŠ¤
                    "kidshankook.kr": "article.article-veiw-body", #93 ì†Œë…„í•œêµ­ì¼ë³´
                    "journalist.or.kr": "div#news_body_area", #94 ê¸°ìí˜‘íšŒë³´
                    "jeollailbo.com": "article.article-veiw-body", #95 ì „ë¼ì¼ë³´
                    "jemin.com": "article.article-veiw-body", #96 ì œë¯¼ì¼ë³´
                    "kukinews.com": "div#articleContent", #97 ì¿ í‚¤ë‰´ìŠ¤
                    "ekn.kr": "div#news_body_area_contents", #98 ì—ë„ˆì§€ê²½ì œ
                    "pttimes.com": "article.article-veiw-body", #99 í‰íƒì‹œë¯¼ì‹ ë¬¸
                    "mediapen.com": "div#articleBody", #100ë¯¸ë””ì–´íœ
                    "koreatimes.com": "div#print_arti", #101ì½”ë¦¬ì•„íƒ€ì„ìŠ¤
                    "okinews.com": "div#article-view-content-div", #102ì˜¥ì²œì‹ ë¬¸
                    "igimpo.com": "article.article-veiw-body", #103ê¹€í¬ì‹ ë¬¸
                    #"gwangnam.co.kr": "div#content", #104ê´‘ë‚¨ì¼ë³´
                    "pdjournal.com": "article.article-veiw-body", #105PDì €ë„
                    "pennmike.com": "article.article-veiw-body", #106íœì•¤ë“œë§ˆì´í¬
                    "hsnews.co.kr": "article.article-veiw-body", #107í™ì„±ì‹ ë¬¸
                    "metroseoul.co.kr": "div.col-12", #108ë©”íŠ¸ë¡œê²½ì œ
                    "pressian.com": "div.article_body", #109í”„ë ˆì‹œì•ˆ
                    "womaneconomy.co.kr": "article.article-veiw-body", #110ì—¬ì„±ê²½ì œì‹ ë¬¸
                    #"wooriy.com": "", #111ì˜ì•”ìš°ë¦¬ì‹ ë¬¸
                    "gynet.co.kr": "div#article-view-content-div", #112ê´‘ì–‘ì‹ ë¬¸
                    "newssc.co.kr": "div#article-view-content-div", #113ë‰´ìŠ¤ì„œì²œ
                    "kidkangwon.co.kr": "div#article-view-content-div", #114ì–´ë¦°ì´ê°•ì›
                    "mygoyang.com": "article.article-veiw-body", #115ì£¼ê°„ê³ ì–‘ì‹ ë¬¸
                    "soraknews.co.kr": "td#ct", #116ì£¼ê°„ì„¤ì•…ì‹ ë¬¸
                    "seoulwire.com": "article.article-veiw-body", #117ì„œìš¸ì™€ì´ì–´
            }

            selector = next((v for k, v in selector_map.items() if k in domain), None)
            if selector:
                div = soup.select_one(selector)
                if div:
                    body = div.get_text(separator="\n", strip=True)
                    if len(body) > 300:
                        return body, driver

            fallback = fallback_with_requests(url)
            return fallback, driver

        except Exception as e:
            log(f"âŒ get_news_article_body ì˜ˆì™¸ ë°œìƒ (ì‹œë„ {attempt+1}): {e}", index)
            kill_driver(driver, index)
            driver = create_driver(index)
            if driver is None:
                log("âŒ ë“œë¼ì´ë²„ ì¬ìƒì„± ì‹¤íŒ¨", index)
                return "", None
            time.sleep(1)

    kill_driver(driver, index)
    fallback = fallback_with_requests(url)
    return fallback, None


def is_excluded(url):
    return any(domain in url for domain in excluded_domains)

MAX_QUERY_LENGTH = 100

def generate_search_queries(title, first, second, last):
    def truncate(text):
        return text[:MAX_QUERY_LENGTH] if text else ""
    # ì…ë ¥ í…ìŠ¤íŠ¸ë“¤ ì •ì œ
    title_clean = truncate(clean_text(title))
    first_clean = truncate(clean_text(first))
    second_clean = truncate(clean_text(second))
    last_clean = truncate(clean_text(last))

    keywords = truncate(extract_keywords(title_clean))

    queries = list(set(filter(None, [
        title_clean,
        # keywords + " " + press,
        first_clean,
        second_clean,
        last_clean
    ])))
    return queries

def extract_oid_from_naver_url(link):
    parsed = urlparse(link)
    path = parsed.path

    # íŒ¨í„´: /article/<oid>/<aid>
    match = re.search(r"/article/(\d{3})/\d+", path)
    if match:
        return match.group(1)

    # ì˜ˆì™¸ì : n.news.naver.com/mnews/article/<oid>/<aid>
    match = re.search(r"/mnews/article/(\d{3})/\d+", path)
    if match:
        return match.group(1)

    return None

def search_news_with_api(queries, driver, client_id, client_secret, max_results=15, index=None):
    headers = {
        "X-Naver-Client-Id": client_id,
        "X-Naver-Client-Secret": client_secret
    }

    results = []
    seen_links = set()

    for q in queries:
        url = f"https://openapi.naver.com/v1/search/news.json?query={urllib.parse.quote(q)}&display={max_results}&sort=sim"
        try:
            res = requests.get(url, headers=headers, timeout=10)
            if res.status_code != 200:
                log(f"âš ï¸ API ê²€ìƒ‰ ì‹¤íŒ¨ ({res.status_code}) - {q}", index)
                continue

            items = res.json().get("items", [])
            for item in items:
                link = item.get("link")
                title = BeautifulSoup(item.get("title", ""), "html.parser").get_text()

                if not link or link in seen_links or is_excluded(link):
                    continue

                if "naver.com" in link:
                    oid = extract_oid_from_naver_url(link)
                    if not oid:
                        log(f"âš ï¸ OID ì¶”ì¶œ ì‹¤íŒ¨ â†’ ìŠ¤í‚µ: {link}", index)
                        continue

                    if "n.news.naver.com" in link:
                        if oid not in trusted_news_oids:
                            # log(f"ğŸš« ë¹„ì‹ íƒ ë‰´ìŠ¤ ì–¸ë¡  (oid={oid}) â†’ {link}", index)
                            continue
                    elif "sports.naver.com" in link:
                        if oid not in trusted_sports_oids:
                            # log(f"ğŸš« ë¹„ì‹ íƒ ìŠ¤í¬ì¸  ì–¸ë¡  (oid={oid}) â†’ {link}", index)
                            continue
                    elif "entertain.naver.com" in link:
                        if oid not in trusted_entertain_oids:
                            # log(f"ğŸš« ë¹„ì‹ íƒ ì—”í„° ì–¸ë¡  (oid={oid}) â†’ {link}", index)
                            continue

                body, new_driver = get_news_article_body(link, driver, index=index)
                if new_driver != driver:
                    log("ğŸ” ë“œë¼ì´ë²„ê°€ ìƒˆë¡œ ê°±ì‹ ë˜ì—ˆìŠµë‹ˆë‹¤", index)
                    driver = new_driver

                seen_links.add(link)
                body, _ = get_news_article_body(link, driver, index=index)
                if body and len(body) > 300:
                    cleaned_body = clean_text(body)
                    results.append({"title": title, "link": link, "body": cleaned_body})

        except Exception as e:
            log(f"âŒ API ê²€ìƒ‰ ì˜¤ë¥˜: {e}", index)
            continue

    return results

def _normalize_for_exact(s: str) -> str:
    s = unescape(s)
    s = re.sub(r'<[^>]+>', ' ', s)              # HTML íƒœê·¸ ì œê±°
    s = s.replace('â€œ','"').replace('â€','"').replace('â€™',"'").replace('â€˜',"'")
    s = s.replace('â€“', '-').replace('â€”', '-')   # ëŒ€ì‹œ í†µì¼
    # ê´„í˜¸ë¥˜ ê°„ë‹¨ í†µì¼
    s = s.replace('ï¼ˆ','(').replace('ï¼‰',')').replace('[','(').replace(']',')')
    # ìˆ«ì ì½¤ë§ˆ ì œê±°(8,060 -> 8060)
    s = re.sub(r'(?<=\d),(?=\d)', '', s)
    # ê³µë°± ì •ë¦¬
    s = re.sub(r'\s+', ' ', s).strip()
    return s

def _split_sentences(text: str):
    text = text.strip()
    if not text:
        return []
    if _HAS_KSS:
        return [s.strip() for s in kss.split_sentences(text) if s and s.strip()]
    # fallback: ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œ + ì¤„ë°”ê¿ˆ
    parts = re.split(r'(?<=[.!?])\s+|\n+', text)
    return [p.strip() for p in parts if p and p.strip()]

def _is_valid_sentence(s: str, min_chars=20, min_tokens=5):
    if len(s) < min_chars:
        return False
    if len(re.findall(r'\w+', s)) < min_tokens:
        return False
    return True

def _almost_equal(a: str, b: str, tol: float = 0.98) -> bool:
    """êµ¬ë‘ì /ê³µë°± ë“± ë¯¸ì„¸ì°¨ì´ë¥¼ í—ˆìš©í•˜ëŠ” 'ê±°ì˜ ì™„ì „ì¼ì¹˜'."""
    return difflib.SequenceMatcher(a=a, b=b, autojunk=False).ratio() >= tol

def exact_copy_rate(article_text: str,
                    post_text: str,
                    mode: str = "sentence",      # "sentence" | "substr" | "hybrid"
                    min_chars: int = 20,
                    min_tokens: int = 5,
                    almost_tol: float = 0.98) -> float:
    """
    ë³µì œìœ¨ = (ì¼ì¹˜(ë˜ëŠ” ê±°ì˜-ì¼ì¹˜) ë¬¸ì¥ ìˆ˜) / (ì›ë¬¸ ìœ íš¨ ë¬¸ì¥ ìˆ˜)
    ë°˜í™˜ê°’ì€ ì†Œìˆ˜ì  ë‘˜ì§¸ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼ (0.00 ~ 1.00)
    - mode="sentence": ë¬¸ì¥â†’ë¬¸ì¥ ì§‘í•© ë§¤ì¹­ + ê±°ì˜-ì¼ì¹˜ ë³´ê°•
    - mode="substr": ì›ë¬¸ ë¬¸ì¥ âˆˆ ê²Œì‹œê¸€ ë³¸ë¬¸ ì„œë¸ŒìŠ¤íŠ¸ë§
    - mode="hybrid": sentence ìš°ì„ , ì‹¤íŒ¨ë¶„ë§Œ substrë¡œ ì¬í™•ì¸
    """
    # 1) ë¬¸ì¥ ë¶„ë¦¬ + ì •ê·œí™”
    A = [_normalize_for_exact(x) for x in _split_sentences(article_text)]
    A = [x for x in A if _is_valid_sentence(x, min_chars, min_tokens)]
    if not A:
        return 0.0

    P_sent = [_normalize_for_exact(x) for x in _split_sentences(post_text)]
    P_set = set(P_sent)
    P_all = _normalize_for_exact(post_text)

    copied = 0

    if mode in ("sentence", "hybrid"):
        # 1ì°¨: ì™„ì „ ì¼ì¹˜
        unmatched = []
        for s in A:
            if s in P_set:
                copied += 1
            else:
                unmatched.append(s)

        # 2ì°¨: ê±°ì˜-ì¼ì¹˜(ë¯¸ì„¸ êµ¬ë‘ì /ê³µë°± ì°¨ì´ í—ˆìš©)
        if unmatched:
            for s in list(unmatched):
                candidates = [t for t in P_sent if abs(len(t) - len(s)) / max(1, len(s)) < 0.2]
                if any(_almost_equal(s, t, tol=almost_tol) for t in candidates):
                    copied += 1
                    unmatched.remove(s)

        # hybrid ëª¨ë“œë©´ ë‚¨ì€ unmatchedë¥¼ substrë¡œ í™•ì¸
        if mode == "hybrid" and unmatched:
            for s in unmatched:
                if s and s in P_all:
                    copied += 1

    elif mode == "substr":
        for s in A:
            if s and s in P_all:
                copied += 1

    total = len(A)
    if total == 0:
        return 0.0
    return round(copied / total, 2)   # âœ… ì†Œìˆ˜ì  ë‘˜ì§¸ìë¦¬ê¹Œì§€ ë°˜ì˜¬ë¦¼

